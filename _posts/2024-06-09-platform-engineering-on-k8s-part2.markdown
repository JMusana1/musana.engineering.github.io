---
layout: post
title: Platform Engineering on Kubernetes with Terraform, Argo, and FastAPI - Part 2
date: 2024-06-9 13:32:20 +0300
description: Explore the practical implementation of Platform Engineering using powerful tools like Terraform, Argo Events, Argo Workflows
img: i-rest.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [platformengineering, kubernetes]
---
With the groundwork established in **[Part 1](https://musana.engineering/platform-engineering-on-k8s-part1/)**, Part 2 dives deeper into the implementation and integration of the key components that will power our platform's capabilities: Argo Events for event-driven automation, Argo Workflows for CI/CD pipelines, and FastAPI as the intuitive frontend API layer. 

## Table of Contents
- [Event-driven Automation ](#event-driven-automation)
  - [EventBus](#eventbus)
  - [EventSource ](#eventsource)
  - [Sensor ](#sensor)

## Event-driven Automation
To add event-driven automation capabilities within our platform, we will configure Argo Events to integrate with an event source and trigger automated Argo workflows. Our aim is to trigger a Workflow upon receiving an HTTP POST request from FastAPI, such as when a developer initiates a request to provision a new environment or deploy an application. To achieve this, we need to create the following resources in Kubernetes.

- ## EventBus
The **[EventBus](https://argoproj.github.io/argo-events/concepts/eventbus/)** resource acts as the transport layer within Argo Events, facilitating the communication between EventSources and Sensors. It serves as a central hub where EventSources publish events, and Sensors subscribe to those events to execute triggers and automated workflows. Argo Events supports three implementations of the EventBus: NATS, Jetstream, and Kafka. In our case, we are using the NATS implementation, which provides a lightweight and efficient messaging system for event distribution.

{% highlight javascript %}
apiVersion: argoproj.io/v1alpha1
kind: EventBus
metadata:
  name: eventbus
  namespace: argo-events
spec:
  nats:
    native:
      // Optional, defaults to 3. If it is < 3, set it to 3, that is the minimal requirement.
      replicas: 3
      // Optional, authen strategy, "none" or "token", defaults to "none"
      auth: token
{% endhighlight %}

In this configuration:
- We define an EventBus resource named eventbus in the argo-events namespace.
- The **spec.nats.native** section specifies that we are using the NATS implementation of the EventBus.
- The replicas field is set to 3, which ensures high availability and fault tolerance by running three replicas of the NATS server.
- The auth field is set to token, enabling token-based authentication for secure communication between EventSources, Sensors, and the EventBus.

To create the EventBus resource, run:
{% highlight javascript %}
kubectl apply -f idp/core/tools/argo/events/sources/eventbus.yaml
{% endhighlight %}

By creating an EventBus, we have established a reliable and secure messaging system that facilitates the flow of events within our platform. EventSources, such as the Webhook we are going to create in the next section, will publish events to the EventBus. Sensors, which we will configure later, will subscribe to specific events and trigger the corresponding Argo Workflows.

- ## EventSource
The **[EventSource](https://argoproj.github.io/argo-events/concepts/event_source/)** resource specifies the configurations required to consume events from external sources, such as webhooks, and transform them into CloudEvents, a standardized format for describing event data. CloudEvents are then dispatched to the event bus for further processing. In our case, we are configuring a Webhook EventSource to listen for incoming HTTP requests on specific endpoints. This will allow us to trigger automated workflows based on events generated by developers interacting with FastAPI.

{% highlight javascript %}
apiVersion: argoproj.io/v1alpha1
kind: EventSource
metadata:
  name: webhook
  namespace: argo-events
spec:
  eventBusName: eventbus
  service:
    ports:
      - port: 12000
        targetPort: 12000
  webhook:
    // Defines the endpoints that will trigger events
    storage:
      port: "12000"
      endpoint: /storage
      method: POST
    compute:
      port: "12000"
      endpoint: /compute
      method: POST
    database:
      port: "12000"
      endpoint: /database
      method: POST
{% endhighlight %}

In this configuration:

- The EventSource is named webhook and resides in the argo-events namespace.
- It is associated with the eventbus event bus, which acts as a central hub for processing and routing events.
- The service section specifies that the Webhook event source will listen on port 12000.
- The webhook section defines the specific endpoints that will trigger events:
  - **/storage endpoint:** This endpoint will trigger an event when an HTTP POST request is received, to provision Azure storage resources such as Blob storage accounts and File shares
  - **/compute endpoint:** This endpoint will trigger an event when an HTTP POST request is received, to provision Azure compute resources such as virtual machines, Kubernetes clusters.
  - **/database endpoint:** This endpoint will trigger an event when an HTTP POST request is received, to provision Azure database resources such as SQL databases, NoSQL databases, or caches.

By defining these endpoints, we have mapped specific API calls from FastAPI to corresponding events within Argo Events. These events will then be processed and used to trigger automated workflows, such as provisioning infrastructure, deploying applications, or configuring application settings, using Argo Workflows. This configuration seamlessly integrates our platform API with Argo Events, enabling event-driven automation.

To create the EventSource resource, run:
{% highlight javascript %}
kubectl apply -f idp/core/tools/argo/events/sources/eventsource.yaml
{% endhighlight %}

- ## Sensor
The **[Sensor](https://argoproj.github.io/argo-events/concepts/sensor/)**  defines a set of event dependencies (inputs) and triggers (outputs). It listens to events on the eventbus and acts as an event dependency manager to resolve and execute the triggers. A dependency is an event the sensor is waiting to happen.Based on the platform capabilities we described in **[Part 1](https://musana.engineering/platform-engineering-on-k8s-part1/)**, we are going to create the following Sensor resources in Argo Events.

- ### Compute Provisioning Sensor
This sensor listens for events from the **/compute** webhook endpoint and triggers an Argo Workflow named **compute-provision**. The workflow executes a series of steps to provision the requested infrastructure resources using Terraform.

{% highlight javascript %}
apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: sensor-compute
  namespace: argo-events
spec:
  eventBusName: eventbus-main
  template:
    serviceAccountName: sa-argo-workflow
  dependencies:
    - name: webhook
      eventSourceName: webhook
      eventName: compute
      eventBusName: eventbus-main
  triggers:
    - template:
        name: terraform
        k8s:
          operation: create
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: compute-provision-
                namespace: argo-events
              spec:
                entrypoint: terraform
                serviceAccountName: sa-argo-workflow
                imagePullSecrets:
                  - name: rpspeastus2acr
                arguments:
                  parameters:   
                    - name: region
                      value: "default" 
                    - name: cloud_provider
                      value: "default"
                    - name: resource_type
                      value: "default"
                    - name: environment
                      value: "default"
                    - name: requester_name
                      value: "default"
                    - name: requester_email
                      value: "default" 
                templates:
                - name: terraform
                  dag: 
                    tasks:
                    - name: terraform-plan
                      templateRef:
                        name: compute-provision-workflow
                        template: plan
                      arguments:
                          parameters:
                            - name: region
                              value: "{{workflow.parameters.region}}"
                            - name: cloud_provider
                              value: "{{workflow.parameters.cloud_provider}}"
                            - name: resource_type
                              value: "{{workflow.parameters.resource_type}}"
                            - name: environment
                              value: "{{workflow.parameters.environment}}"
                            - name: requester_name
                              value: "{{workflow.parameters.requester_name}}"
                            - name: requester_email
                              value: "{{workflow.parameters.requester_email}}"

                    # ... removed for brevity ...

                    - name: terraform-apply
                      templateRef:
                        name: compute-provision-workflow
                        template: apply
                      arguments:
                          parameters:
                            - name: region
                              value: "{{workflow.parameters.region}}"
                            - name: cloud_provider
                              value: "{{workflow.parameters.cloud_provider}}"
                            - name: resource_type
                              value: "{{workflow.parameters.resource_type}}"
                            - name: environment
                              value: "{{workflow.parameters.environment}}"
                            - name: requester_name
                              value: "{{workflow.parameters.requester_name}}"
                            - name: requester_email
                              value: "{{workflow.parameters.requester_email}}"
                      dependencies: ["terraform-plan"]

          parameters:
            - src:
                dependencyName: webhook
                dataKey: body.region
              dest: spec.arguments.parameters.0.value
            - src:
                dependencyName: webhook
                dataKey: body.cloud_provider
              dest: spec.arguments.parameters.1.value
            - src:
                dependencyName: webhook
                dataKey: body.resource_type
              dest: spec.arguments.parameters.2.value
            - src:
                dependencyName: webhook
                dataKey: body.environment
              dest: spec.arguments.parameters.3.value
            - src:
                dependencyName: webhook
                dataKey: body.requester_name
              dest: spec.arguments.parameters.4.value
            - src:
                dependencyName: webhook
                dataKey: body.requester_email
              dest: spec.arguments.parameters.5.value
{% endhighlight %}
- ### Storage Provisioning Sensor
This sensor listens for events from the **/storage** webhook endpoint and triggers an Argo Workflow named **storage-provision**. The workflow executes a series of steps to provision the requested storage infrastructure resources using Terraform.

- ### Database Provisioning Sensor
This Sensor listens for events from the **/database** webhook endpoint and triggers an Argo Workflow named **database-provision** when such an event is received. The workflow executes steps to provision the requested database infrastructure resources using the database_config payload from the event.

To create all the sensors described above, run:
{% highlight javascript %}
kubectl apply -f idp/core/tools/argo/events/sensors
{% endhighlight %}


## More coming shortly...!