With the groundwork established in **[Part 1](https://musana.engineering/platform-engineering-on-k8s-part1/), Part 2 dives deeper into the implementation and integration of the key components that will power our platform's capabilities: Argo Events for event-driven automation, Argo Workflows for CI/CD pipelines, and FastAPI as the intuitive frontend API layer. 

## Table of Contents
- [Event-driven Automation ](#event-driven-automation)
  - [Platform Capabilities](#platform-capabilities)
  - [Platform Tools ](#platform-tools)
  - [Prerequisites ](#prerequisites)
- [Implementation ](#implementation)
  - [Foundation Architecture ](#foundation-architecture)
  - [Core network ](#core-network)
  - [Core Kubernetes ](#core-kubernetes)
  - [Core Tools ](#core-tools)
  - [Certificate Management ](#certificate-management)
  - [Secret Management ](#secret-management)
  - [Ingress Management ](#ingress-management)
- [Summary ](#summary)

## Event-driven Automation
To add event-driven automation capabilities within our platform, let's configure Argo Events to integrate with an event source and trigger automated Argo workflows. Our aim is to trigger a Workflow upon receiving an HTTP POST request from FastAPI, such as when a developer initiates a request to provision a new environment or deploy an application. To achieve this, we need to create the following resources in Kubernetes.

**[EventBus](https://argoproj.github.io/argo-events/concepts/eventbus/)**, acts as the transport layer within Argo Events, facilitating the communication between EventSources and Sensors. It serves as a central hub where EventSources publish events, and Sensors subscribe to those events to execute triggers and automated workflows. Argo Events supports three implementations of the EventBus: NATS, Jetstream, and Kafka.
 
In our case, we are using the NATS implementation, which provides a lightweight and efficient messaging system for event distribution.

{% highlight javascript %}
apiVersion: argoproj.io/v1alpha1
kind: EventBus
metadata:
  name: eventbus
  namespace: argo-events
spec:
  nats:
    native:
      # Optional, defaults to 3. If it is < 3, set it to 3, that is the minimal requirement.
      replicas: 3
      # Optional, authen strategy, "none" or "token", defaults to "none"
      auth: token
{% endhighlight %}

In this configuration:
- We define an EventBus resource named eventbus in the argo-events namespace.
- The spec.nats.native section specifies that we are using the NATS implementation of the EventBus.
- The replicas field is set to 3, which ensures high availability and fault tolerance by running three replicas of the NATS server.
- The auth field is set to token, enabling token-based authentication for secure communication between EventSources, Sensors, and the EventBus.

To create the EventBus resource, run:
{% highlight javascript %}
kubectl apply -f idp/core/tools/events/eventbus.yaml
{% endhighlight %}

By configuring the EventBus, we establish a reliable and secure messaging system that facilitates the flow of events within our internal developer platform. EventSources, such as the Webhook event source we defined earlier, will publish events to the EventBus. Sensors, which we will configure later, will subscribe to specific events and trigger the corresponding Argo Workflows.

**[EventSource](https://argoproj.github.io/argo-events/concepts/event_source/)** The EventSource resource specifies the configurations required to consume events from external sources, such as webhooks, and transform them into CloudEvents, a standardized format for describing event data. CloudEvents are then dispatched to the event bus for further processing. In our case, we are configuring a Webhook EventSource to listen for incoming HTTP requests on specific endpoints. This allows us to trigger automated workflows based on events generated by developers interacting with the internal developer platform's APIs.

{% highlight javascript %}
apiVersion: argoproj.io/v1alpha1
kind: EventSource
metadata:
  name: webhook
  namespace: argo-events
spec:
  eventBusName: eventbus
  service:
    ports:
      - port: 12000
        targetPort: 12000
  webhook:
    // Defines the endpoints that will trigger events
    environments:
      port: "12000"
      endpoint: /environments
      method: POST
    applications:
      port: "12000"
      endpoint: /applications
      method: POST
    configuration:
      port: "12000"
      endpoint: /configuration
      method: POST
{% endhighlight %}

In this configuration:

- The EventSource is named webhook and resides in the argo-events namespace.
- It is associated with the eventbus event bus, which acts as a central hub for processing and routing events.
- The service section specifies that the Webhook event source will listen on port 12000.
- The webhook section defines the specific endpoints that will trigger events:
  - /environments: This endpoint will trigger an event when an HTTP POST request is received for provisioning a new environment.
  - /applications: This endpoint will trigger an event when an HTTP POST request is received for deploying an application.
  - /configuration: This endpoint will trigger an event when an HTTP POST request is received for configuring an application

To create the EventSource resource, run:
{% highlight javascript %}
kubectl apply -f idp/core/tools/events/eventsource.yaml
{% endhighlight %}

By defining these endpoints, we have mapped specific API calls from FastAPI to corresponding events within Argo Events. These events will then be processed and used to trigger automated workflows, such as provisioning infrastructure, deploying applications, or configuring application settings, using Argo Workflows. This configuration seamlessly integrates our platform API with Argo Events, enabling event-driven automation.

A **[Sensor](https://argoproj.github.io/argo-events/concepts/sensor/)** defines a set of event dependencies (inputs) and triggers (outputs). It listens to events on the eventbus and acts as an event dependency manager to resolve and execute the triggers.

{% highlight javascript %}

{% endhighlight %}

A Webhook sensor acts as an event listener. It waits for waiting for incoming HTTP requests and translating them into CloudEvents, a standardized format for describing event data. Once a CloudEvent is generated, Argo Events can trigger the corresponding workflow based on the event payload and predefined rules.
Setting up the Webhook sensor involves the following steps:

- **Define the Event Source:** We will create an EventSource resource in Kubernetes, specifying the type as webhook and configuring the necessary parameters, such as the port and route for receiving incoming requests.

- **Configure the Sensor:** Next, we will define a Sensor resource that listens for events from the specified EventSource. The Sensor will define the rules and conditions for triggering the desired Argo Workflow based on the received event data. Integrate with Argo Workflows: Within the Sensor configuration, we will specify the Argo Workflow to be triggered, along with any required parameters or event payloads to be passed to the workflow.

Based on the provided content, here are three examples of sensors we can create for this demonstration, representing three different things developers might order on our internal developer platform:

## Argo Workflows

In this section, we’ll lay down the groundwork necessary for exploring Snowpark Container Services. We will use Docker to create an OCI-compliant image to deploy to Snowpark. We’ll start by creating Snowflake objects

Since Argo Workflows executes tasks using containers, we need to prepare a container image that includes Terraform alongside all the other tools we will need such as Azure CLI, Curl, Wget etc. In this section, we will create a Dockerfile, build and push our image to the container registry from where Argo Workflows will pull from.

To build the Docker image containing the required tools, we'll start by creating a Dockerfile with the necessary instructions:

{% highlight javascript %}
// Navigate to the aks directory
cd idp/core/tools/cicd
{% endhighlight %}


## Part 2 is coming shortly... Let me cook!